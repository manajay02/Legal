# ============================================================
# CivilModel - Environment Configuration
# ============================================================
# Copy this file to .env and fill in your values
# ============================================================

# Application Settings
APP_NAME=CivilCaseExtractor
APP_VERSION=1.0.0
DEBUG=True
LOG_LEVEL=INFO

# API Configuration
API_V1_PREFIX=/api/v1
HOST=0.0.0.0
PORT=8000

# ============================================================
# TESSERACT OCR CONFIGURATION
# ============================================================
# Windows default: C:\Program Files\Tesseract-OCR\tesseract.exe
# macOS (brew): /usr/local/bin/tesseract
# Linux: /usr/bin/tesseract
TESSERACT_PATH=C:\Program Files\Tesseract-OCR\tesseract.exe
TESSERACT_LANGS=eng+sin
TESSERACT_DPI=300

# ============================================================
# LLM PROVIDER CONFIGURATION
# ============================================================
# Choose ONE provider:
#   - "openrouter" = Cloud API (easy, no GPU needed) [RECOMMENDED]
#   - "ollama"     = Local (requires GPU + merged model)
# ============================================================
LLM_PROVIDER=openrouter
LLM_TIMEOUT=120
LLM_MAX_RETRIES=3

# ------------------------------------------------------------
# OPTION 1: OpenRouter (Recommended for easy setup)
# ------------------------------------------------------------
# Get API key from: https://openrouter.ai/keys
# DeepSeek is powerful and very cheap ($0.14/1M tokens)
OPENROUTER_API_KEY=your-openrouter-api-key-here
OPENROUTER_MODEL=deepseek/deepseek-chat

# ------------------------------------------------------------
# OPTION 2: Ollama (For running trained model locally)
# ------------------------------------------------------------
# Requires: GPU with 8GB+ VRAM, Ollama installed
# Steps:
#   1. Run merge_adapter.py to merge LoRA with base model
#   2. Import into Ollama: ollama create civilmodel-qwen3b -f Modelfile
#   3. Start Ollama: ollama serve
#   4. Change LLM_PROVIDER=ollama above
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=civilmodel-qwen3b

# ============================================================
# DATABASE CONFIGURATION
# ============================================================
DATABASE_URL=sqlite+aiosqlite:///./data/civil_cases.db

# ============================================================
# DIRECTORY PATHS
# ============================================================
DATA_DIR=./data
UPLOAD_DIR=./data/uploads
PROCESSED_DIR=./data/processed
TRAINING_DIR=./data/training
SAMPLE_CASES_DIR=./data/sample_cases

# ============================================================
# FILE UPLOAD SETTINGS
# ============================================================
MAX_UPLOAD_SIZE_MB=50
ALLOWED_EXTENSIONS=pdf

# ============================================================
# PROCESSING SETTINGS
# ============================================================
MAX_PAGES_PER_DOCUMENT=500
PARALLEL_PAGE_PROCESSING=4

# ============================================================
# CORS SETTINGS (for frontend integration)
# ============================================================
CORS_ORIGINS=["http://localhost:3000", "http://localhost:8080", "http://localhost:5173"]
CORS_ALLOW_CREDENTIALS=True
CORS_ALLOW_METHODS=["*"]
CORS_ALLOW_HEADERS=["*"]
