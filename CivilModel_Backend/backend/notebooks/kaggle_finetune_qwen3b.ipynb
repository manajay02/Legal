{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d167cf9",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Installation\n",
    "\n",
    "Install Unsloth and all required dependencies for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a98b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth for efficient fine-tuning\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Install compatible PEFT version (let pip resolve dependencies)\n",
    "!pip install \"peft>=0.13.0\" --upgrade\n",
    "\n",
    "# Install TRL and dependencies WITHOUT overwriting PEFT\n",
    "!pip install --no-deps trl accelerate bitsandbytes\n",
    "\n",
    "# Additional dependencies\n",
    "!pip install datasets\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02736a6",
   "metadata": {},
   "source": [
    "## Part 2: Load the Base Model\n",
    "\n",
    "Load Qwen2.5-3B-Instruct with 4-bit quantization for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 4096  # Adjust based on your document lengths\n",
    "dtype = None  # Auto-detect (float16 for P100)\n",
    "load_in_4bit = True  # Use 4-bit quantization for memory efficiency\n",
    "\n",
    "# Load the pre-quantized Qwen2.5-3B model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Model: Qwen2.5-3B-Instruct (4-bit)\")\n",
    "print(f\"   Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters for efficient fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank (higher = more parameters, better quality)\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Optimized for inference\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
    "    random_state=42,\n",
    "    # Removed use_rslora and loftq_config for Kaggle compatibility\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters added!\")\n",
    "print(f\"   Trainable parameters: {model.print_trainable_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e78bb",
   "metadata": {},
   "source": [
    "## Part 3: Data Preparation\n",
    "\n",
    "Load the training dataset and format it for the Qwen chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaceec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# IMPORTANT: Update this path to your Kaggle dataset location\n",
    "# ============================================================\n",
    "DATASET_PATH = \"/kaggle/input/civilmodel-training-data/training_data.jsonl\"\n",
    "\n",
    "print(f\"Loading dataset from: {DATASET_PATH}\")\n",
    "print(f\"File size: {os.path.getsize(DATASET_PATH) / 1024:.2f} KB\")\n",
    "\n",
    "# Load JSONL manually (older datasets library doesn't support lines=True parameter)\n",
    "raw_data = []\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:  # Skip empty lines\n",
    "            continue\n",
    "        try:\n",
    "            raw_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö† Warning: Skipping line {line_num} due to JSON error: {e}\")\n",
    "            continue\n",
    "\n",
    "if len(raw_data) == 0:\n",
    "    raise ValueError(\"No valid JSON data found in dataset file!\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded!\")\n",
    "print(f\"   Total examples: {len(dataset)}\")\n",
    "print(f\"   Columns: {dataset.column_names}\")\n",
    "\n",
    "# Preview first example\n",
    "print(f\"\\nüìÑ First example preview:\")\n",
    "print(json.dumps(dataset[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbcec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Qwen chat template\n",
    "QWEN_CHAT_TEMPLATE = \"\"\"<|im_start|>system\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant}<|im_end|>\"\"\"\n",
    "\n",
    "# System prompt for legal document extraction\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert Sri Lankan paralegal. Your task is to meticulously extract metadata and content from the provided OCR text of a Supreme Court judgment and format it as a single, valid JSON object.\n",
    "\n",
    "RULES:\n",
    "1. Return ONLY the JSON object. No explanatory text.\n",
    "2. Do NOT invent data. Use null for missing fields.\n",
    "3. parties and judges must be flat lists of strings.\n",
    "4. Divide content into logical sections.\"\"\"\n",
    "\n",
    "\n",
    "def format_example(example):\n",
    "    \"\"\"\n",
    "    Format a single training example into the Qwen chat template.\n",
    "    \n",
    "    Dataset format (from training_data.jsonl):\n",
    "    {\n",
    "        \"metadata\": {...},\n",
    "        \"sections\": [...]\n",
    "    }\n",
    "    \n",
    "    We reconstruct the input text from sections and use the full structure as output.\n",
    "    \"\"\"\n",
    "    # Reconstruct the input text from sections content\n",
    "    sections = example.get('sections', [])\n",
    "    input_text = \"\\n\\n\".join([section.get('content', '') for section in sections])\n",
    "    \n",
    "    # The full example (metadata + sections) is the expected output\n",
    "    output_data = {\n",
    "        \"metadata\": example.get('metadata', {}),\n",
    "        \"sections\": example.get('sections', [])\n",
    "    }\n",
    "    assistant_output = json.dumps(output_data, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Format with the chat template\n",
    "    formatted = QWEN_CHAT_TEMPLATE.format(\n",
    "        system=SYSTEM_PROMPT,\n",
    "        user=input_text,\n",
    "        assistant=assistant_output\n",
    "    )\n",
    "    \n",
    "    return {\"text\": formatted}\n",
    "\n",
    "\n",
    "# Apply formatting to the dataset\n",
    "formatted_dataset = dataset.map(\n",
    "    format_example,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Formatting dataset\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset formatted!\")\n",
    "print(f\"   Total examples: {len(formatted_dataset)}\")\n",
    "\n",
    "# Preview a formatted example\n",
    "print(f\"\\nüìÑ Formatted example preview:\")\n",
    "print(formatted_dataset[0]['text'][:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Split dataset for validation (recommended for >50 examples)\n",
    "# This helps prevent overfitting\n",
    "\n",
    "if len(formatted_dataset) > 50:\n",
    "    # Split 90/10 train/validation\n",
    "    split_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = split_dataset['train']\n",
    "    eval_dataset = split_dataset['test']\n",
    "    \n",
    "    print(f\"‚úÖ Dataset split:\")\n",
    "    print(f\"   Training: {len(train_dataset)} examples\")\n",
    "    print(f\"   Validation: {len(eval_dataset)} examples\")\n",
    "else:\n",
    "    # Too small to split - use all for training\n",
    "    train_dataset = formatted_dataset\n",
    "    eval_dataset = None\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è Dataset too small to split ({len(formatted_dataset)} examples)\")\n",
    "    print(f\"   Using all examples for training\")\n",
    "    print(f\"   Risk of overfitting - consider reducing epochs to 1-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff966e8",
   "metadata": {},
   "source": [
    "## Part 4: Fine-Tuning\n",
    "\n",
    "Configure and run the SFTTrainer with optimized settings for P100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cf938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import os\n",
    "\n",
    "# Disable Triton optimizations for P100 GPU (CUDA capability 6.0 < 7.0 required)\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "# Training configuration optimized for Kaggle P100 (16GB VRAM)\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=\"./civilmodel_outputs\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    # ADJUST EPOCHS BASED ON DATASET SIZE:\n",
    "    # - 10-50 examples: 1-2 epochs (lower risk of overfitting)\n",
    "    # - 50-100 examples: 2-3 epochs\n",
    "    # - 100+ examples: 3-5 epochs\n",
    "    num_train_epochs=2,  # Change to 3-5 if you have >100 examples\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
    "    \n",
    "    # Optimizer settings\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    \n",
    "    # Memory optimization for P100\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Evaluation (if eval_dataset is provided)\n",
    "    eval_strategy=\"epoch\" if eval_dataset is not None else \"no\",\n",
    "    load_best_model_at_end=True if eval_dataset is not None else False,\n",
    "    metric_for_best_model=\"eval_loss\" if eval_dataset is not None else None,\n",
    "    \n",
    "    # Disable torch.compile for P100 compatibility\n",
    "    torch_compile=False,\n",
    "    \n",
    "    # Other\n",
    "    seed=42,\n",
    "    report_to=\"none\",  # Disable wandb for now\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured!\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Validation: {'Enabled' if eval_dataset is not None else 'Disabled'}\")\n",
    "print(f\"   Triton/torch.compile: Disabled (P100 compatibility)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c053aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # None if dataset too small\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Set to True for shorter sequences to improve efficiency\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SFTTrainer initialized!\")\n",
    "print(f\"   Training examples: {len(train_dataset)}\")\n",
    "if eval_dataset:\n",
    "    print(f\"   Validation examples: {len(eval_dataset)}\")\n",
    "print(f\"   Max sequence length: {max_seq_length}\")\n",
    "\n",
    "# Estimate training time\n",
    "estimated_time_per_epoch = (len(train_dataset) / training_args.per_device_train_batch_size / training_args.gradient_accumulation_steps) * 2.5  # seconds\n",
    "total_estimated_time = estimated_time_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Estimated training time:\")\n",
    "print(f\"   Per epoch: ~{estimated_time_per_epoch:.1f} seconds ({estimated_time_per_epoch/60:.2f} minutes)\")\n",
    "print(f\"   Total ({training_args.num_train_epochs} epochs): ~{total_estimated_time:.1f} seconds ({total_estimated_time/60:.2f} minutes)\")\n",
    "print(f\"   If 5 epochs: ~{estimated_time_per_epoch * 5 / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Max memory: {max_memory} GB\")\n",
    "print(f\"Reserved memory: {start_gpu_memory} GB\")\n",
    "print(f\"\")\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING FINE-TUNING...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ RUN THE TRAINING!\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
    "print(f\"Final loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"Samples per second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final GPU memory usage\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "\n",
    "print(f\"\\nüìä GPU Memory Usage:\")\n",
    "print(f\"   Peak reserved memory: {used_memory} GB\")\n",
    "print(f\"   Memory used for training: {used_memory_for_training} GB\")\n",
    "print(f\"   Memory utilization: {used_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb7490",
   "metadata": {},
   "source": [
    "## Part 5: Save the Model\n",
    "\n",
    "Save the fine-tuned LoRA adapter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a402e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "MODEL_SAVE_PATH = \"./civilmodel_qwen3b_v1\"\n",
    "\n",
    "model.save_pretrained(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# List saved files\n",
    "import os\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "for f in os.listdir(MODEL_SAVE_PATH):\n",
    "    size = os.path.getsize(os.path.join(MODEL_SAVE_PATH, f)) / 1024 / 1024\n",
    "    print(f\"   {f}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping merged model - using adapter-only for HuggingFace upload\n",
    "print(\"‚ÑπÔ∏è Merged model skipped - adapter-only approach for HuggingFace\")\n",
    "print(\"   Users will load: unsloth/Qwen2.5-3B-Instruct-bnb-4bit + your adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b0d74",
   "metadata": {},
   "source": [
    "## Part 6: Package for Download\n",
    "\n",
    "Create a ZIP file containing the adapter model for easy download and HuggingFace upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping inference test - packaging adapter for HuggingFace upload\n",
    "import shutil\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üì¶ Preparing adapter model for download...\")\n",
    "print(f\"   Adapter path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"   Files: {len(os.listdir(MODEL_SAVE_PATH))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b884b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZIP file of the adapter model\n",
    "ZIP_PATH = \"/kaggle/working/civilmodel_qwen3b_v1_adapter.zip\"\n",
    "\n",
    "print(\"üóúÔ∏è Creating ZIP file...\")\n",
    "with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(MODEL_SAVE_PATH):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, os.path.dirname(MODEL_SAVE_PATH))\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"   Added: {arcname}\")\n",
    "\n",
    "zip_size = os.path.getsize(ZIP_PATH) / 1024 / 1024\n",
    "print(f\"\\n‚úÖ ZIP created: {ZIP_PATH}\")\n",
    "print(f\"   Size: {zip_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and download instructions\n",
    "print(\"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"   Final Loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"   Training Time: {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
    "print(f\"   Examples Trained: {len(train_dataset)}\")\n",
    "print(f\"\\nüì¶ Download:\")\n",
    "print(f\"   File: civilmodel_qwen3b_v1_adapter.zip\")\n",
    "print(f\"   Location: /kaggle/working/\")\n",
    "print(f\"   Size: {zip_size:.2f} MB\")\n",
    "print(f\"\\nü§ó HuggingFace Upload Instructions:\")\n",
    "print(\"   1. Download the ZIP file from Kaggle output\")\n",
    "print(\"   2. Extract it locally\")\n",
    "print(\"   3. Upload to HuggingFace Hub using:\")\n",
    "print(\"      huggingface-cli upload your-username/civilmodel-qwen3b-lora ./civilmodel_qwen3b_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example for loading the adapter\n",
    "print(\"\\nüí° How to use this adapter:\")\n",
    "print(\"\"\"\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length=4096,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Load your adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    \"your-username/civilmodel-qwen3b-lora\"\n",
    ")\n",
    "\n",
    "# Set to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70521c",
   "metadata": {},
   "source": [
    "## Part 7: Final Summary\n",
    "\n",
    "Packaging complete - download the ZIP file from /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final file locations\n",
    "print(\"=\"*60)\n",
    "print(\"üìÅ FILES READY FOR DOWNLOAD\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úÖ Adapter ZIP: /kaggle/working/civilmodel_qwen3b_v1_adapter.zip\")\n",
    "print(f\"   Size: {zip_size:.2f} MB\")\n",
    "print(f\"   Contains: LoRA adapter + tokenizer + config\")\n",
    "print(f\"\\n‚úÖ Training checkpoints: ./civilmodel_outputs/\")\n",
    "print(f\"   (Optional - only needed if you want to resume training)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Ready to upload to HuggingFace!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c448fc5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "### 1. Download from Kaggle\n",
    "- Download `civilmodel_qwen3b_v1_adapter.zip` from `/kaggle/working/`\n",
    "- Extract locally\n",
    "\n",
    "### 2. Upload to HuggingFace Hub\n",
    "```bash\n",
    "# Install huggingface-cli\n",
    "pip install huggingface_hub\n",
    "\n",
    "# Login\n",
    "huggingface-cli login\n",
    "\n",
    "# Upload adapter\n",
    "huggingface-cli upload your-username/civilmodel-qwen3b-lora ./civilmodel_qwen3b_v1\n",
    "```\n",
    "\n",
    "### 3. Use in Production\n",
    "```python\n",
    "# Load base model + your adapter\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length=4096,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    \"your-username/civilmodel-qwen3b-lora\"\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Adapter-only approach: ~130MB upload instead of 6GB! üéâ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
